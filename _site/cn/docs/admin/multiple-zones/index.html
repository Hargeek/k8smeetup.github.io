<h2 id="介绍">介绍</h2>

<p>Kubernetes 从v1.2开始支持将集群运行在多个故障域中。
(GCE 中称其为 “区（Zones）”， AWS 中称其为 “可用区（Availability Zones）”，这里我们也称其为 “区”)。
它是广泛意义上的集群联邦特性的轻量级版本 (之前被称为 <a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/multicluster/federation.md">“Ubernetes”</a>)。
完整的集群联邦能够将多个分别运行在不同区或云供应商（或本地数据中心）的集群集中管理。
然而，很多用户只是希望通过将单一云供应商上的Kubernetes集群运行在多个区域，来提高集群的可用性，
这就是1.2版本中提供的对多区域的支持。
(之前被称为 “Ubernetes Lite”)。</p>

<p>多区域的支持是有明确限制的： Kubernetes集群能够运行在多个区，但必须在同一个地域内 (云供应商也须一致)。
目前只有GCE和AWS自动支持 (尽管在其他云甚至裸机上，也很容易通过为节点和卷添加合适的标签来实现类似的支持)。</p>

<ul id="markdown-toc">
  <li><a href="#介绍" id="markdown-toc-介绍">介绍</a></li>
  <li><a href="#功能" id="markdown-toc-功能">功能</a></li>
  <li><a href="#限制" id="markdown-toc-限制">限制</a></li>
  <li><a href="#演练" id="markdown-toc-演练">演练</a>    <ul>
      <li><a href="#创建集群" id="markdown-toc-创建集群">创建集群</a></li>
      <li><a href="#标记节点" id="markdown-toc-标记节点">标记节点</a></li>
      <li><a href="#添加其它区中的节点" id="markdown-toc-添加其它区中的节点">添加其它区中的节点</a></li>
      <li><a href="#卷的亲和性" id="markdown-toc-卷的亲和性">卷的亲和性</a></li>
      <li><a href="#pod的跨区域分布" id="markdown-toc-pod的跨区域分布">Pod的跨区域分布</a></li>
      <li><a href="#停止集群" id="markdown-toc-停止集群">停止集群</a></li>
    </ul>
  </li>
</ul>

<h2 id="功能">功能</h2>

<p>节点启动时，Kubelet自动为其添加区信息的标签。</p>

<p>在单一区域的集群中，Kubernetes 会自动将副本管理器或服务的pod分布到各节点上 (以减轻单实例故障的影响)。
在多区域的集群中，这种分布的行为扩展到了区域级别
(以减少区域故障对整体的影响)。  (通过 <code class="highlighter-rouge">SelectorSpreadPriority</code> 来实现)。
这种分发是尽力而为（best-effort）的，所以如果集群在各个区之间是异构的
(比如，各区间的节点数量不同、节点类型不同、pod的资源需求不同等)可能导致pod无法完全均匀地分布。
如果需要的话，用户可以使用同质的区(节点数量和节点类型相同)来减少区域之间分配不均匀的可能。</p>

<p>当卷被创建时， <code class="highlighter-rouge">PersistentVolumeLabel</code>准入控制器会自动为其添加区域的标签。
调度器 (通过 <code class="highlighter-rouge">VolumeZonePredicate</code> 断言) 会确申领该卷的pod被调度到该卷对应的区域，
因为卷是不支持跨区挂载的。</p>

<h2 id="限制">限制</h2>

<p>对多区的支持有一些重要的限制：</p>

<ul>
  <li>
    <p>我们假设不同的区域间在网络上离得很近，所以我们不做任何的区域感知路由。 特别是，通过服务的网络访问可能跨区域 (即使该服务后端pod的其中一些运行在与客户端相同的区域中)，这可能导致额外的延迟和损耗。</p>
  </li>
  <li>
    <p>卷的区域亲和性只对 <code class="highlighter-rouge">PersistentVolume</code>有效。 例如，如果你在pod的spec中直接指定一个EBS的卷，则不会生效。</p>
  </li>
  <li>
    <p>集群不支持跨云平台或地域 (这些功能需要完整的集群联邦特性支持)。</p>
  </li>
  <li>
    <p>尽管节点位于多区域，目前默认情况下 kube-up 创建的管理节点是单实例的。 所以尽管服务是高可用的，并且能够容忍跨区域的性能损耗，管理平面还是单区域的。 需要高可用的管理平面的用户可以按照 <a href="/docs/admin/high-availability">高可用</a> 指导来操作。</p>
  </li>
  <li>
    <p>目前StatefulSet的卷动态创建时的跨区域分配，与pod的亲和性/反亲和性不兼容。</p>
  </li>
  <li>
    <p>StatefulSet的名称包含破折号 (“-“)时，可能影响到卷在区域间的均匀分布。</p>
  </li>
  <li>
    <p>为deployment或pod指定多个PVC时，要求其StorageClass处于同一区域内，否则，相应的PV卷需要在一个区域中静态配置。 另一种方式是使用StatefulSet，这可以确保同一副本所挂载的卷位于同一区内。</p>
  </li>
</ul>

<h2 id="演练">演练</h2>

<p>接下来我们将介绍如何同时在 GCE 和 AWS 上创建和使用多区域的集群。 为此，你需要创建一个完整的集群
(指定 <code class="highlighter-rouge">MULTIZONE=true</code>)，然后再次执行 <code class="highlighter-rouge">kube-up</code>（指定 <code class="highlighter-rouge">KUBE_USE_EXISTING_MASTER=true</code>）来添加其他区域的节点。</p>

<h3 id="创建集群">创建集群</h3>

<p>按正常方式创建集群，但是传入 MULTIZONE 来通知集群对多区域进行管理。 在 us-central1-a 区域创建节点。</p>

<p>GCE:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl <span class="nt">-sS</span> https://get.k8s.io | <span class="nv">MULTIZONE</span><span class="o">=</span><span class="nb">true </span><span class="nv">KUBERNETES_PROVIDER</span><span class="o">=</span>gce <span class="nv">KUBE_GCE_ZONE</span><span class="o">=</span>us-central1-a <span class="nv">NUM_NODES</span><span class="o">=</span>3 bash
</code></pre></div></div>

<p>AWS:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl <span class="nt">-sS</span> https://get.k8s.io | <span class="nv">MULTIZONE</span><span class="o">=</span><span class="nb">true </span><span class="nv">KUBERNETES_PROVIDER</span><span class="o">=</span>aws <span class="nv">KUBE_AWS_ZONE</span><span class="o">=</span>us-west-2a <span class="nv">NUM_NODES</span><span class="o">=</span>3 bash
</code></pre></div></div>

<p>该步骤按正常方式创建了集群，仍然运行在单个区域中。
但 <code class="highlighter-rouge">MULTIZONE=true</code> 已经开启了多区域的能力。</p>

<h3 id="标记节点">标记节点</h3>

<p>查看节点，你可以发现节点上打了区域信息的标签。
节点位于 <code class="highlighter-rouge">us-central1-a</code> (GCE) 或者 <code class="highlighter-rouge">us-west-2a</code> (AWS)。 标签 <code class="highlighter-rouge">failure-domain.beta.kubernetes.io/region</code> 用于区分地域，
标签 <code class="highlighter-rouge">failure-domain.beta.kubernetes.io/zone</code> 用于区分区域。</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;</span> kubectl get nodes <span class="nt">--show-labels</span>


NAME                     STATUS                     AGE   VERSION          LABELS
kubernetes-master        Ready,SchedulingDisabled   6m    v1.6.0+fff5156   beta.kubernetes.io/instance-type<span class="o">=</span>n1-standard-1,failure-domain.beta.kubernetes.io/region<span class="o">=</span>us-central1,failure-domain.beta.kubernetes.io/zone<span class="o">=</span>us-central1-a,kubernetes.io/hostname<span class="o">=</span>kubernetes-master
kubernetes-minion-87j9   Ready                      6m    v1.6.0+fff5156   beta.kubernetes.io/instance-type<span class="o">=</span>n1-standard-2,failure-domain.beta.kubernetes.io/region<span class="o">=</span>us-central1,failure-domain.beta.kubernetes.io/zone<span class="o">=</span>us-central1-a,kubernetes.io/hostname<span class="o">=</span>kubernetes-minion-87j9
kubernetes-minion-9vlv   Ready                      6m    v1.6.0+fff5156   beta.kubernetes.io/instance-type<span class="o">=</span>n1-standard-2,failure-domain.beta.kubernetes.io/region<span class="o">=</span>us-central1,failure-domain.beta.kubernetes.io/zone<span class="o">=</span>us-central1-a,kubernetes.io/hostname<span class="o">=</span>kubernetes-minion-9vlv
kubernetes-minion-a12q   Ready                      6m    v1.6.0+fff5156   beta.kubernetes.io/instance-type<span class="o">=</span>n1-standard-2,failure-domain.beta.kubernetes.io/region<span class="o">=</span>us-central1,failure-domain.beta.kubernetes.io/zone<span class="o">=</span>us-central1-a,kubernetes.io/hostname<span class="o">=</span>kubernetes-minion-a12q
</code></pre></div></div>

<h3 id="添加其它区中的节点">添加其它区中的节点</h3>

<p>接下来我们复用已有的管理节点，添加运行于其它区域 （us-central1-b或us-west-2b）中的节点。
再次执行 kube-up， 通过指定 <code class="highlighter-rouge">KUBE_USE_EXISTING_MASTER=true</code>，
kube-up 不会创建新的管理节点，而是会复用之前创建的。</p>

<p>GCE:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">KUBE_USE_EXISTING_MASTER</span><span class="o">=</span><span class="nb">true </span><span class="nv">MULTIZONE</span><span class="o">=</span><span class="nb">true </span><span class="nv">KUBERNETES_PROVIDER</span><span class="o">=</span>gce <span class="nv">KUBE_GCE_ZONE</span><span class="o">=</span>us-central1-b <span class="nv">NUM_NODES</span><span class="o">=</span>3 kubernetes/cluster/kube-up.sh
</code></pre></div></div>

<p>在 AWS 中我们还需要为新增的子网指定网络CIDR，还有管理节点的内部IP地址。</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">KUBE_USE_EXISTING_MASTER</span><span class="o">=</span><span class="nb">true </span><span class="nv">MULTIZONE</span><span class="o">=</span><span class="nb">true </span><span class="nv">KUBERNETES_PROVIDER</span><span class="o">=</span>aws <span class="nv">KUBE_AWS_ZONE</span><span class="o">=</span>us-west-2b <span class="nv">NUM_NODES</span><span class="o">=</span>3 <span class="nv">KUBE_SUBNET_CIDR</span><span class="o">=</span>172.20.1.0/24 <span class="nv">MASTER_INTERNAL_IP</span><span class="o">=</span>172.20.0.9 kubernetes/cluster/kube-up.sh
</code></pre></div></div>

<p>再次查看节点，3个新增的节点已经启动，并被标记为us-central1-b：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;</span> kubectl get nodes <span class="nt">--show-labels</span>

NAME                     STATUS                     AGE   VERSION           LABELS
kubernetes-master        Ready,SchedulingDisabled   16m   v1.6.0+fff5156    beta.kubernetes.io/instance-type<span class="o">=</span>n1-standard-1,failure-domain.beta.kubernetes.io/region<span class="o">=</span>us-central1,failure-domain.beta.kubernetes.io/zone<span class="o">=</span>us-central1-a,kubernetes.io/hostname<span class="o">=</span>kubernetes-master
kubernetes-minion-281d   Ready                      2m    v1.6.0+fff5156    beta.kubernetes.io/instance-type<span class="o">=</span>n1-standard-2,failure-domain.beta.kubernetes.io/region<span class="o">=</span>us-central1,failure-domain.beta.kubernetes.io/zone<span class="o">=</span>us-central1-b,kubernetes.io/hostname<span class="o">=</span>kubernetes-minion-281d
kubernetes-minion-87j9   Ready                      16m   v1.6.0+fff5156    beta.kubernetes.io/instance-type<span class="o">=</span>n1-standard-2,failure-domain.beta.kubernetes.io/region<span class="o">=</span>us-central1,failure-domain.beta.kubernetes.io/zone<span class="o">=</span>us-central1-a,kubernetes.io/hostname<span class="o">=</span>kubernetes-minion-87j9
kubernetes-minion-9vlv   Ready                      16m   v1.6.0+fff5156    beta.kubernetes.io/instance-type<span class="o">=</span>n1-standard-2,failure-domain.beta.kubernetes.io/region<span class="o">=</span>us-central1,failure-domain.beta.kubernetes.io/zone<span class="o">=</span>us-central1-a,kubernetes.io/hostname<span class="o">=</span>kubernetes-minion-9vlv
kubernetes-minion-a12q   Ready                      17m   v1.6.0+fff5156    beta.kubernetes.io/instance-type<span class="o">=</span>n1-standard-2,failure-domain.beta.kubernetes.io/region<span class="o">=</span>us-central1,failure-domain.beta.kubernetes.io/zone<span class="o">=</span>us-central1-a,kubernetes.io/hostname<span class="o">=</span>kubernetes-minion-a12q
kubernetes-minion-pp2f   Ready                      2m    v1.6.0+fff5156    beta.kubernetes.io/instance-type<span class="o">=</span>n1-standard-2,failure-domain.beta.kubernetes.io/region<span class="o">=</span>us-central1,failure-domain.beta.kubernetes.io/zone<span class="o">=</span>us-central1-b,kubernetes.io/hostname<span class="o">=</span>kubernetes-minion-pp2f
kubernetes-minion-wf8i   Ready                      2m    v1.6.0+fff5156    beta.kubernetes.io/instance-type<span class="o">=</span>n1-standard-2,failure-domain.beta.kubernetes.io/region<span class="o">=</span>us-central1,failure-domain.beta.kubernetes.io/zone<span class="o">=</span>us-central1-b,kubernetes.io/hostname<span class="o">=</span>kubernetes-minion-wf8i
</code></pre></div></div>

<h3 id="卷的亲和性">卷的亲和性</h3>

<p>使用动态创建卷的功能创建一个卷 (只有PV持久卷才支持区域亲和性)：</p>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">kubectl</span><span class="w"> </span><span class="err">create</span><span class="w"> </span><span class="err">-f</span><span class="w"> </span><span class="err">-</span><span class="w"> </span><span class="err">&lt;&lt;EOF</span><span class="w">
</span><span class="p">{</span><span class="w">
  </span><span class="s2">"kind"</span><span class="p">:</span><span class="w"> </span><span class="s2">"PersistentVolumeClaim"</span><span class="p">,</span><span class="w">
  </span><span class="s2">"apiVersion"</span><span class="p">:</span><span class="w"> </span><span class="s2">"v1"</span><span class="p">,</span><span class="w">
  </span><span class="s2">"metadata"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="s2">"name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"claim1"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"annotations"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
        </span><span class="s2">"volume.alpha.kubernetes.io/storage-class"</span><span class="p">:</span><span class="w"> </span><span class="s2">"foo"</span><span class="w">
    </span><span class="p">}</span><span class="w">
  </span><span class="p">},</span><span class="w">
  </span><span class="s2">"spec"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="s2">"accessModes"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
      </span><span class="s2">"ReadWriteOnce"</span><span class="w">
    </span><span class="p">],</span><span class="w">
    </span><span class="s2">"resources"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
      </span><span class="s2">"requests"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
        </span><span class="s2">"storage"</span><span class="p">:</span><span class="w"> </span><span class="s2">"5Gi"</span><span class="w">
      </span><span class="p">}</span><span class="w">
    </span><span class="p">}</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">
</span><span class="err">EOF</span><span class="w">
</span></code></pre></div></div>

<p><strong>注意：</strong> Kubernetes 1.3以上的版本中可以将PVC分发到多个已配置的区域中，在1.2版本中， 动态卷只能创建在管理节点所在的区域内(即这里的 us-central1-a / us-west-2a)；相关issue
(<a href="https://github.com/kubernetes/kubernetes/issues/23330">#23330</a>)
在1.3后续的版本中已解决。</p>

<p>现在我们验证一下 Kubernetes 自动为创建的PV打上了所在地域和区域的标签。</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;</span> kubectl get pv <span class="nt">--show-labels</span>
NAME           CAPACITY   ACCESSMODES   STATUS    CLAIM            REASON    AGE       LABELS
pv-gce-mj4gm   5Gi        RWO           Bound     default/claim1             46s       failure-domain.beta.kubernetes.io/region<span class="o">=</span>us-central1,failure-domain.beta.kubernetes.io/zone<span class="o">=</span>us-central1-a
</code></pre></div></div>

<p>现在我们将创建使用这些PVC的pod。
因为 GCE 的PD存储 / AWS 的EBS 卷 不支持跨区域挂载，
这意味着相应的pod只能创建在卷所在的区域中。</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">kubectl create -f - &lt;&lt;EOF</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Pod</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">mypod</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">containers</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">myfrontend</span>
      <span class="na">image</span><span class="pi">:</span> <span class="s">nginx</span>
      <span class="na">volumeMounts</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">mountPath</span><span class="pi">:</span> <span class="s2">"</span><span class="s">/var/www/html"</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">mypd</span>
  <span class="na">volumes</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">mypd</span>
      <span class="na">persistentVolumeClaim</span><span class="pi">:</span>
        <span class="na">claimName</span><span class="pi">:</span> <span class="s">claim1</span>
<span class="s">EOF</span>
</code></pre></div></div>

<p>注意pod被自动创建在了卷所在的区域中，因为云供应商通常不支持卷的跨区域挂载（attach）。</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;</span> kubectl describe pod mypod | <span class="nb">grep </span>Node
Node:        kubernetes-minion-9vlv/10.240.0.5
<span class="o">&gt;</span> kubectl get node kubernetes-minion-9vlv <span class="nt">--show-labels</span>
NAME                     STATUS    AGE    VERSION          LABELS
kubernetes-minion-9vlv   Ready     22m    v1.6.0+fff5156   beta.kubernetes.io/instance-type<span class="o">=</span>n1-standard-2,failure-domain.beta.kubernetes.io/region<span class="o">=</span>us-central1,failure-domain.beta.kubernetes.io/zone<span class="o">=</span>us-central1-a,kubernetes.io/hostname<span class="o">=</span>kubernetes-minion-9vlv
</code></pre></div></div>

<h3 id="pod的跨区域分布">Pod的跨区域分布</h3>

<p>副本管理器或服务的pod被自动创建在了不同的区域。  首先，在第三个区域内启动节点：</p>

<p>GCE:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">KUBE_USE_EXISTING_MASTER</span><span class="o">=</span><span class="nb">true </span><span class="nv">MULTIZONE</span><span class="o">=</span><span class="nb">true </span><span class="nv">KUBERNETES_PROVIDER</span><span class="o">=</span>gce <span class="nv">KUBE_GCE_ZONE</span><span class="o">=</span>us-central1-f <span class="nv">NUM_NODES</span><span class="o">=</span>3 kubernetes/cluster/kube-up.sh
</code></pre></div></div>

<p>AWS:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">KUBE_USE_EXISTING_MASTER</span><span class="o">=</span><span class="nb">true </span><span class="nv">MULTIZONE</span><span class="o">=</span><span class="nb">true </span><span class="nv">KUBERNETES_PROVIDER</span><span class="o">=</span>aws <span class="nv">KUBE_AWS_ZONE</span><span class="o">=</span>us-west-2c <span class="nv">NUM_NODES</span><span class="o">=</span>3 <span class="nv">KUBE_SUBNET_CIDR</span><span class="o">=</span>172.20.2.0/24 <span class="nv">MASTER_INTERNAL_IP</span><span class="o">=</span>172.20.0.9 kubernetes/cluster/kube-up.sh
</code></pre></div></div>

<p>验证你现在在3个区域内拥有节点:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl get nodes <span class="nt">--show-labels</span>
</code></pre></div></div>

<p>创建 guestbook-go 示例应用， 它包含一个副本数为3的RC，运行一个简单的网络应用：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>find kubernetes/examples/guestbook-go/ <span class="nt">-name</span> <span class="s1">'*.json'</span> | xargs <span class="nt">-I</span> <span class="o">{}</span> kubectl create <span class="nt">-f</span> <span class="o">{}</span>
</code></pre></div></div>

<p>Pod应该分布在全部3个区域上：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;</span>  kubectl describe pod <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>guestbook | <span class="nb">grep </span>Node
Node:        kubernetes-minion-9vlv/10.240.0.5
Node:        kubernetes-minion-281d/10.240.0.8
Node:        kubernetes-minion-olsh/10.240.0.11

 <span class="o">&gt;</span> kubectl get node kubernetes-minion-9vlv kubernetes-minion-281d kubernetes-minion-olsh <span class="nt">--show-labels</span>
NAME                     STATUS    AGE    VERSION          LABELS
kubernetes-minion-9vlv   Ready     34m    v1.6.0+fff5156   beta.kubernetes.io/instance-type<span class="o">=</span>n1-standard-2,failure-domain.beta.kubernetes.io/region<span class="o">=</span>us-central1,failure-domain.beta.kubernetes.io/zone<span class="o">=</span>us-central1-a,kubernetes.io/hostname<span class="o">=</span>kubernetes-minion-9vlv
kubernetes-minion-281d   Ready     20m    v1.6.0+fff5156   beta.kubernetes.io/instance-type<span class="o">=</span>n1-standard-2,failure-domain.beta.kubernetes.io/region<span class="o">=</span>us-central1,failure-domain.beta.kubernetes.io/zone<span class="o">=</span>us-central1-b,kubernetes.io/hostname<span class="o">=</span>kubernetes-minion-281d
kubernetes-minion-olsh   Ready     3m     v1.6.0+fff5156   beta.kubernetes.io/instance-type<span class="o">=</span>n1-standard-2,failure-domain.beta.kubernetes.io/region<span class="o">=</span>us-central1,failure-domain.beta.kubernetes.io/zone<span class="o">=</span>us-central1-f,kubernetes.io/hostname<span class="o">=</span>kubernetes-minion-olsh
</code></pre></div></div>

<p>负载平衡器覆盖集群中的所有区域； guestbook-go 示例包含一个
负载均衡服务的例子：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;</span> kubectl describe service guestbook | <span class="nb">grep </span>LoadBalancer.Ingress
LoadBalancer Ingress:   130.211.126.21

<span class="o">&gt;</span> <span class="nv">ip</span><span class="o">=</span>130.211.126.21

<span class="o">&gt;</span> curl <span class="nt">-s</span> http://<span class="k">${</span><span class="nv">ip</span><span class="k">}</span>:3000/env | <span class="nb">grep </span>HOSTNAME
  <span class="s2">"HOSTNAME"</span>: <span class="s2">"guestbook-44sep"</span>,

<span class="o">&gt;</span> <span class="o">(</span><span class="k">for </span>i <span class="k">in</span> <span class="sb">`</span>seq 20<span class="sb">`</span><span class="p">;</span> <span class="k">do </span>curl <span class="nt">-s</span> http://<span class="k">${</span><span class="nv">ip</span><span class="k">}</span>:3000/env | <span class="nb">grep </span>HOSTNAME<span class="p">;</span> <span class="k">done</span><span class="o">)</span>  | sort | uniq
  <span class="s2">"HOSTNAME"</span>: <span class="s2">"guestbook-44sep"</span>,
  <span class="s2">"HOSTNAME"</span>: <span class="s2">"guestbook-hum5n"</span>,
  <span class="s2">"HOSTNAME"</span>: <span class="s2">"guestbook-ppm40"</span>,
</code></pre></div></div>

<p>负载平衡器正确指向了所有的pod，即使它们位于不同的区域内。</p>

<h3 id="停止集群">停止集群</h3>

<p>使用完成后，进行清理：</p>

<p>GCE:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">KUBERNETES_PROVIDER</span><span class="o">=</span>gce <span class="nv">KUBE_USE_EXISTING_MASTER</span><span class="o">=</span><span class="nb">true </span><span class="nv">KUBE_GCE_ZONE</span><span class="o">=</span>us-central1-f kubernetes/cluster/kube-down.sh
<span class="nv">KUBERNETES_PROVIDER</span><span class="o">=</span>gce <span class="nv">KUBE_USE_EXISTING_MASTER</span><span class="o">=</span><span class="nb">true </span><span class="nv">KUBE_GCE_ZONE</span><span class="o">=</span>us-central1-b kubernetes/cluster/kube-down.sh
<span class="nv">KUBERNETES_PROVIDER</span><span class="o">=</span>gce <span class="nv">KUBE_GCE_ZONE</span><span class="o">=</span>us-central1-a kubernetes/cluster/kube-down.sh
</code></pre></div></div>

<p>AWS:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">KUBERNETES_PROVIDER</span><span class="o">=</span>aws <span class="nv">KUBE_USE_EXISTING_MASTER</span><span class="o">=</span><span class="nb">true </span><span class="nv">KUBE_AWS_ZONE</span><span class="o">=</span>us-west-2c kubernetes/cluster/kube-down.sh
<span class="nv">KUBERNETES_PROVIDER</span><span class="o">=</span>aws <span class="nv">KUBE_USE_EXISTING_MASTER</span><span class="o">=</span><span class="nb">true </span><span class="nv">KUBE_AWS_ZONE</span><span class="o">=</span>us-west-2b kubernetes/cluster/kube-down.sh
<span class="nv">KUBERNETES_PROVIDER</span><span class="o">=</span>aws <span class="nv">KUBE_AWS_ZONE</span><span class="o">=</span>us-west-2a kubernetes/cluster/kube-down.sh
</code></pre></div></div>
